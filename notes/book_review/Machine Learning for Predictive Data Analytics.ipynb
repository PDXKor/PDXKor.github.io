{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee44b2e",
   "metadata": {},
   "source": [
    "## Authors:\n",
    "\n",
    "* John D. Kelleher\n",
    "* Brian Mac Namee\n",
    "* Aoife D'Arcy\n",
    "\n",
    "https://machinelearningbook.com/\n",
    "https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics\n",
    "\n",
    "\n",
    "## Chapter 1 - Machine Learning for Predictive Analytics\n",
    "\n",
    "\"Machine learning algorithims work by searching through a set of possible prediction models for the model that best captures the relationship between the descriptive features and target feature in a dataset.\" *page - 7*\n",
    "\n",
    "\"If a predictive model is to be useful, it must be able to make predictions for queries that are not present in the data. A prediction model that makes the correct predictions for these queries caputres the underlying relationship between the descriptive and target features and is said to **generalize** well. Indeed, the goal of machine learning is to find the predicive model that generalizes best. In order to find this single best model, a machine learning algorithim must use some criteria for choosing among the candidate models it considers during a search.\" - *page 11*\n",
    "\n",
    "As mentioned, generalization with low error rate, be it MSE or some other prediction loss scoring technique is the ultimate goal. In application model generalization usually has to do with under-fitting or over-fitting test data, and how one sets up their test and training data, including how big of a sample of population data they actually have.\n",
    "\n",
    "### Terms\n",
    "(just a subset of terms in the chapter)\n",
    "\n",
    "* **Ill-poised problem** - A problem for which a unique solution cannot be determined using only the information that is available. \n",
    "\n",
    "* **Inductive bias** - The set set of assumptions that defines the model selection criteria of a machine learning algorithim.\n",
    "\n",
    "* **Restriction bias** - Constrains the set of models that the algorithim will consider during the learning process.\n",
    "\n",
    "* **Preference bias** - Guides the learning algorithim to prefer certain models over others.\n",
    "\n",
    "\n",
    "## Chapter 2 - Data Insights to Decisions\n",
    "\n",
    "The authors describe organizing input feature data and target features into a single tabluar data structure called the Analytics Base Table or ABT. Different terminology but similar to creating a matrix where each row is an input vector sometimes denoted as X and a y matrix where each row is an target feature vector. \n",
    "\n",
    "Most of this chapter is a basic refresher of how to compile data for model development, any experienced Data Scientist should be able to skim through this chapter, although how the authors structure the language around feature creation is very handy, and can be used to explain to an organization what steps are included in feature development. \n",
    "\n",
    "### Terms\n",
    "(just a subset of terms in the chapter)\n",
    "\n",
    "* **Analytics Base Table** - A simple, flat, tabular data structure made up of rows and columns. The columns are devided into a set of descriptive features and the target feature, and represents an instance of which the prediction can be made.\n",
    "\n",
    "\n",
    "\n",
    "## Chapter 3 - Getting to Know the Data\n",
    "\n",
    "\n",
    "* A Normal Distribution is also considered unimodal and is said to have symmetry.\n",
    "\n",
    "\n",
    "* Multi-modal has many peaks, bi-modal has a single peak, and is cause for concern when running predictions. Multi-modal data usually indicates a different classification of data that is not be extracted.\n",
    "\n",
    "\n",
    "* For each feature in the ABT create a metric to determine the percentage of missing values, a general rule of thumb described is that any feature with data missing in excess of 60% should be disregarded.  \n",
    "\n",
    "\n",
    "* Check irregular cardinality, or uniquness, for instance if a categorical feature is all of the same value, it will provide no value as an input feature and should  be disrteagarded. Issues may also occur with large amounts of cardinality. \n",
    "\n",
    "\n",
    "* For outliers, look at the clamp transformation to see if it is an effective way to mitigate the effect. \n",
    "\n",
    "\n",
    "* Visualize \n",
    "\n",
    "\n",
    "### Terms\n",
    "(just a subset of terms in the chapter)\n",
    "\n",
    "* **Clamp Transformation** - A method for cleaning up outliers that sets the outlier to a max or min threshold value. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f940109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
